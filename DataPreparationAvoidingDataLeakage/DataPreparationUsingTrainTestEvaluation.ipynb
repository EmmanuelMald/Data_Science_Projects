{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Data Leakage\n",
    "\n",
    "#### Emmanuel Amador Maldonado\n",
    "\n",
    "**The main objective is to see the difference in the model performance between using data preparation in the whole dataset before split it into test and train sets, and using data preparation after splitting the dataset into train and test sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model using Train-Test Evaluation\n",
    "\n",
    "In this section, we'll evaluate a logistic regression model using train and test sets on a synthetic binary classification dataset where the inpyt variables have been normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **nake_classification** function in *sklearn.datasets*, we'll create a dataset with a binary target variable\n",
    "\n",
    "- n_samples: Total number of rows\n",
    "- n_features: Number of independent variables (n_features = n_informative + n_redundant + n_repeated)\n",
    "    - n_informative: Number of features that it gives real information related to the target variable\n",
    "    - n_redundant: Number of features that doesn't give real information related to the target variable \n",
    "    - n_repreated: Number of features repeated\n",
    "- random_state: Number of the seed. To make the results replicables\n",
    "- n_classes: Number of labels of the classification problem (Number of labels for the target variable) -> default: 2\n",
    "\n",
    "\n",
    "Returns X, y -> independent variables, and the target variable, respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples = 1000, n_features = 20, n_informative = 15, n_redundant = 5, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.292995</td>\n",
       "      <td>-4.212231</td>\n",
       "      <td>-1.288332</td>\n",
       "      <td>-2.178498</td>\n",
       "      <td>-0.645277</td>\n",
       "      <td>2.580977</td>\n",
       "      <td>0.284224</td>\n",
       "      <td>-7.182793</td>\n",
       "      <td>-1.912111</td>\n",
       "      <td>2.737295</td>\n",
       "      <td>0.813957</td>\n",
       "      <td>3.969737</td>\n",
       "      <td>-2.669398</td>\n",
       "      <td>3.346923</td>\n",
       "      <td>4.197918</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>-0.302019</td>\n",
       "      <td>-4.431706</td>\n",
       "      <td>-2.826467</td>\n",
       "      <td>0.449168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.068399</td>\n",
       "      <td>5.518841</td>\n",
       "      <td>11.238977</td>\n",
       "      <td>-5.039700</td>\n",
       "      <td>-2.086784</td>\n",
       "      <td>2.149685</td>\n",
       "      <td>0.559734</td>\n",
       "      <td>15.113777</td>\n",
       "      <td>-3.071834</td>\n",
       "      <td>-2.574584</td>\n",
       "      <td>3.324576</td>\n",
       "      <td>2.067542</td>\n",
       "      <td>-5.249258</td>\n",
       "      <td>-2.154500</td>\n",
       "      <td>4.931091</td>\n",
       "      <td>1.296735</td>\n",
       "      <td>-3.186133</td>\n",
       "      <td>-3.089948</td>\n",
       "      <td>1.190299</td>\n",
       "      <td>1.620256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.731616</td>\n",
       "      <td>-0.684686</td>\n",
       "      <td>-0.981742</td>\n",
       "      <td>-2.552465</td>\n",
       "      <td>-5.270308</td>\n",
       "      <td>-1.561498</td>\n",
       "      <td>-1.169269</td>\n",
       "      <td>-2.104087</td>\n",
       "      <td>-1.131139</td>\n",
       "      <td>4.654775</td>\n",
       "      <td>-2.786596</td>\n",
       "      <td>-2.034761</td>\n",
       "      <td>2.149657</td>\n",
       "      <td>-0.134154</td>\n",
       "      <td>-1.198231</td>\n",
       "      <td>-2.720604</td>\n",
       "      <td>-0.123961</td>\n",
       "      <td>5.654297</td>\n",
       "      <td>-0.646599</td>\n",
       "      <td>-3.156530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.309107</td>\n",
       "      <td>-0.320548</td>\n",
       "      <td>-6.591664</td>\n",
       "      <td>1.070525</td>\n",
       "      <td>-4.418769</td>\n",
       "      <td>1.134274</td>\n",
       "      <td>2.340813</td>\n",
       "      <td>-5.983425</td>\n",
       "      <td>0.675917</td>\n",
       "      <td>-1.007879</td>\n",
       "      <td>-0.761441</td>\n",
       "      <td>6.866297</td>\n",
       "      <td>1.442270</td>\n",
       "      <td>1.768678</td>\n",
       "      <td>5.173661</td>\n",
       "      <td>-1.070164</td>\n",
       "      <td>-2.447064</td>\n",
       "      <td>-1.109038</td>\n",
       "      <td>-2.997035</td>\n",
       "      <td>1.993212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.488406</td>\n",
       "      <td>-3.213065</td>\n",
       "      <td>1.100805</td>\n",
       "      <td>-1.356223</td>\n",
       "      <td>5.325086</td>\n",
       "      <td>0.729179</td>\n",
       "      <td>-0.257040</td>\n",
       "      <td>-1.035284</td>\n",
       "      <td>0.478013</td>\n",
       "      <td>-0.010764</td>\n",
       "      <td>-0.227408</td>\n",
       "      <td>2.551456</td>\n",
       "      <td>0.951594</td>\n",
       "      <td>-2.914910</td>\n",
       "      <td>-2.186843</td>\n",
       "      <td>-1.089129</td>\n",
       "      <td>1.406454</td>\n",
       "      <td>3.082424</td>\n",
       "      <td>0.925835</td>\n",
       "      <td>-2.326362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.156687</td>\n",
       "      <td>-2.491359</td>\n",
       "      <td>-0.319048</td>\n",
       "      <td>-0.180767</td>\n",
       "      <td>-5.161745</td>\n",
       "      <td>0.536021</td>\n",
       "      <td>-1.435684</td>\n",
       "      <td>0.708005</td>\n",
       "      <td>-2.480216</td>\n",
       "      <td>1.017607</td>\n",
       "      <td>0.899322</td>\n",
       "      <td>2.455431</td>\n",
       "      <td>0.878080</td>\n",
       "      <td>0.619633</td>\n",
       "      <td>1.120065</td>\n",
       "      <td>1.784713</td>\n",
       "      <td>-3.079556</td>\n",
       "      <td>4.192865</td>\n",
       "      <td>-4.931510</td>\n",
       "      <td>0.221128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.095598</td>\n",
       "      <td>5.155741</td>\n",
       "      <td>4.846930</td>\n",
       "      <td>1.677064</td>\n",
       "      <td>-5.461116</td>\n",
       "      <td>2.922476</td>\n",
       "      <td>-4.679053</td>\n",
       "      <td>2.916699</td>\n",
       "      <td>-1.501298</td>\n",
       "      <td>0.243174</td>\n",
       "      <td>0.537594</td>\n",
       "      <td>-12.232591</td>\n",
       "      <td>-1.480634</td>\n",
       "      <td>1.653768</td>\n",
       "      <td>1.381391</td>\n",
       "      <td>-3.410147</td>\n",
       "      <td>-0.437894</td>\n",
       "      <td>6.170876</td>\n",
       "      <td>0.536590</td>\n",
       "      <td>-2.258617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.482795</td>\n",
       "      <td>1.159066</td>\n",
       "      <td>0.805299</td>\n",
       "      <td>-3.453292</td>\n",
       "      <td>-9.464460</td>\n",
       "      <td>-3.631272</td>\n",
       "      <td>-0.010151</td>\n",
       "      <td>3.080085</td>\n",
       "      <td>1.621198</td>\n",
       "      <td>1.300982</td>\n",
       "      <td>2.147708</td>\n",
       "      <td>-0.036937</td>\n",
       "      <td>-0.277505</td>\n",
       "      <td>-0.506086</td>\n",
       "      <td>-5.903622</td>\n",
       "      <td>1.555044</td>\n",
       "      <td>0.679583</td>\n",
       "      <td>3.990227</td>\n",
       "      <td>-4.600461</td>\n",
       "      <td>2.687322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-4.548827</td>\n",
       "      <td>1.388299</td>\n",
       "      <td>1.854430</td>\n",
       "      <td>1.201001</td>\n",
       "      <td>-5.795249</td>\n",
       "      <td>-0.654437</td>\n",
       "      <td>0.537701</td>\n",
       "      <td>1.920046</td>\n",
       "      <td>-0.550207</td>\n",
       "      <td>-2.581467</td>\n",
       "      <td>-2.397156</td>\n",
       "      <td>7.739894</td>\n",
       "      <td>3.841829</td>\n",
       "      <td>-0.280951</td>\n",
       "      <td>-0.937141</td>\n",
       "      <td>1.817031</td>\n",
       "      <td>-3.414837</td>\n",
       "      <td>5.196090</td>\n",
       "      <td>-3.094033</td>\n",
       "      <td>1.823347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.641345</td>\n",
       "      <td>-1.285319</td>\n",
       "      <td>0.959277</td>\n",
       "      <td>-1.510647</td>\n",
       "      <td>7.076031</td>\n",
       "      <td>-1.491739</td>\n",
       "      <td>-0.409155</td>\n",
       "      <td>-3.735169</td>\n",
       "      <td>0.113359</td>\n",
       "      <td>2.438390</td>\n",
       "      <td>3.767352</td>\n",
       "      <td>-6.095996</td>\n",
       "      <td>-1.295550</td>\n",
       "      <td>-0.655873</td>\n",
       "      <td>0.921460</td>\n",
       "      <td>-2.270914</td>\n",
       "      <td>2.296910</td>\n",
       "      <td>-2.945498</td>\n",
       "      <td>-0.863316</td>\n",
       "      <td>-2.363958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.517704</td>\n",
       "      <td>-0.816360</td>\n",
       "      <td>-9.601974</td>\n",
       "      <td>3.063419</td>\n",
       "      <td>-0.206767</td>\n",
       "      <td>3.427853</td>\n",
       "      <td>4.472281</td>\n",
       "      <td>-11.700568</td>\n",
       "      <td>0.354013</td>\n",
       "      <td>-1.511854</td>\n",
       "      <td>-0.047627</td>\n",
       "      <td>8.053843</td>\n",
       "      <td>1.238134</td>\n",
       "      <td>0.816615</td>\n",
       "      <td>5.150886</td>\n",
       "      <td>-2.249004</td>\n",
       "      <td>-0.294797</td>\n",
       "      <td>-1.836554</td>\n",
       "      <td>-1.385168</td>\n",
       "      <td>3.488666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.578265</td>\n",
       "      <td>6.335031</td>\n",
       "      <td>13.688271</td>\n",
       "      <td>-1.759311</td>\n",
       "      <td>5.184626</td>\n",
       "      <td>3.591914</td>\n",
       "      <td>-0.080637</td>\n",
       "      <td>-0.336176</td>\n",
       "      <td>1.711741</td>\n",
       "      <td>3.282520</td>\n",
       "      <td>8.347846</td>\n",
       "      <td>-8.223997</td>\n",
       "      <td>-1.422867</td>\n",
       "      <td>-0.089538</td>\n",
       "      <td>3.273017</td>\n",
       "      <td>-2.760325</td>\n",
       "      <td>1.566113</td>\n",
       "      <td>-2.771605</td>\n",
       "      <td>2.968455</td>\n",
       "      <td>1.412744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-2.145484</td>\n",
       "      <td>-1.672974</td>\n",
       "      <td>-0.183422</td>\n",
       "      <td>-2.307678</td>\n",
       "      <td>11.477898</td>\n",
       "      <td>-1.239618</td>\n",
       "      <td>0.852235</td>\n",
       "      <td>-6.647052</td>\n",
       "      <td>-1.253824</td>\n",
       "      <td>1.836148</td>\n",
       "      <td>4.529539</td>\n",
       "      <td>-5.376926</td>\n",
       "      <td>-1.366454</td>\n",
       "      <td>-0.632935</td>\n",
       "      <td>1.614968</td>\n",
       "      <td>-3.573758</td>\n",
       "      <td>5.654563</td>\n",
       "      <td>-7.952127</td>\n",
       "      <td>1.674620</td>\n",
       "      <td>-1.006084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.641875</td>\n",
       "      <td>3.194960</td>\n",
       "      <td>-10.560793</td>\n",
       "      <td>2.471885</td>\n",
       "      <td>-1.691838</td>\n",
       "      <td>-1.197527</td>\n",
       "      <td>2.901170</td>\n",
       "      <td>-2.877372</td>\n",
       "      <td>1.534504</td>\n",
       "      <td>0.559240</td>\n",
       "      <td>-2.938001</td>\n",
       "      <td>-0.165236</td>\n",
       "      <td>-0.953041</td>\n",
       "      <td>-1.781131</td>\n",
       "      <td>4.306025</td>\n",
       "      <td>-3.230074</td>\n",
       "      <td>-1.223786</td>\n",
       "      <td>0.429879</td>\n",
       "      <td>0.768187</td>\n",
       "      <td>1.594597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.461111</td>\n",
       "      <td>-3.719436</td>\n",
       "      <td>-8.380107</td>\n",
       "      <td>-2.745835</td>\n",
       "      <td>-8.501479</td>\n",
       "      <td>-1.603518</td>\n",
       "      <td>0.815392</td>\n",
       "      <td>-5.369283</td>\n",
       "      <td>0.064245</td>\n",
       "      <td>-0.906424</td>\n",
       "      <td>-3.042304</td>\n",
       "      <td>7.819834</td>\n",
       "      <td>-0.400451</td>\n",
       "      <td>1.879275</td>\n",
       "      <td>-3.935016</td>\n",
       "      <td>2.851727</td>\n",
       "      <td>2.746354</td>\n",
       "      <td>0.898704</td>\n",
       "      <td>-6.315267</td>\n",
       "      <td>1.004400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1          2         3          4         5         6   \\\n",
       "0   0.292995 -4.212231  -1.288332 -2.178498  -0.645277  2.580977  0.284224   \n",
       "1  -0.068399  5.518841  11.238977 -5.039700  -2.086784  2.149685  0.559734   \n",
       "2   0.731616 -0.684686  -0.981742 -2.552465  -5.270308 -1.561498 -1.169269   \n",
       "3   2.309107 -0.320548  -6.591664  1.070525  -4.418769  1.134274  2.340813   \n",
       "4  -0.488406 -3.213065   1.100805 -1.356223   5.325086  0.729179 -0.257040   \n",
       "5  -0.156687 -2.491359  -0.319048 -0.180767  -5.161745  0.536021 -1.435684   \n",
       "6   3.095598  5.155741   4.846930  1.677064  -5.461116  2.922476 -4.679053   \n",
       "7   1.482795  1.159066   0.805299 -3.453292  -9.464460 -3.631272 -0.010151   \n",
       "8  -4.548827  1.388299   1.854430  1.201001  -5.795249 -0.654437  0.537701   \n",
       "9  -0.641345 -1.285319   0.959277 -1.510647   7.076031 -1.491739 -0.409155   \n",
       "10  2.517704 -0.816360  -9.601974  3.063419  -0.206767  3.427853  4.472281   \n",
       "11  1.578265  6.335031  13.688271 -1.759311   5.184626  3.591914 -0.080637   \n",
       "12 -2.145484 -1.672974  -0.183422 -2.307678  11.477898 -1.239618  0.852235   \n",
       "13  3.641875  3.194960 -10.560793  2.471885  -1.691838 -1.197527  2.901170   \n",
       "14  0.461111 -3.719436  -8.380107 -2.745835  -8.501479 -1.603518  0.815392   \n",
       "\n",
       "           7         8         9         10         11        12        13  \\\n",
       "0   -7.182793 -1.912111  2.737295  0.813957   3.969737 -2.669398  3.346923   \n",
       "1   15.113777 -3.071834 -2.574584  3.324576   2.067542 -5.249258 -2.154500   \n",
       "2   -2.104087 -1.131139  4.654775 -2.786596  -2.034761  2.149657 -0.134154   \n",
       "3   -5.983425  0.675917 -1.007879 -0.761441   6.866297  1.442270  1.768678   \n",
       "4   -1.035284  0.478013 -0.010764 -0.227408   2.551456  0.951594 -2.914910   \n",
       "5    0.708005 -2.480216  1.017607  0.899322   2.455431  0.878080  0.619633   \n",
       "6    2.916699 -1.501298  0.243174  0.537594 -12.232591 -1.480634  1.653768   \n",
       "7    3.080085  1.621198  1.300982  2.147708  -0.036937 -0.277505 -0.506086   \n",
       "8    1.920046 -0.550207 -2.581467 -2.397156   7.739894  3.841829 -0.280951   \n",
       "9   -3.735169  0.113359  2.438390  3.767352  -6.095996 -1.295550 -0.655873   \n",
       "10 -11.700568  0.354013 -1.511854 -0.047627   8.053843  1.238134  0.816615   \n",
       "11  -0.336176  1.711741  3.282520  8.347846  -8.223997 -1.422867 -0.089538   \n",
       "12  -6.647052 -1.253824  1.836148  4.529539  -5.376926 -1.366454 -0.632935   \n",
       "13  -2.877372  1.534504  0.559240 -2.938001  -0.165236 -0.953041 -1.781131   \n",
       "14  -5.369283  0.064245 -0.906424 -3.042304   7.819834 -0.400451  1.879275   \n",
       "\n",
       "          14        15        16        17        18        19  \n",
       "0   4.197918  0.999910 -0.302019 -4.431706 -2.826467  0.449168  \n",
       "1   4.931091  1.296735 -3.186133 -3.089948  1.190299  1.620256  \n",
       "2  -1.198231 -2.720604 -0.123961  5.654297 -0.646599 -3.156530  \n",
       "3   5.173661 -1.070164 -2.447064 -1.109038 -2.997035  1.993212  \n",
       "4  -2.186843 -1.089129  1.406454  3.082424  0.925835 -2.326362  \n",
       "5   1.120065  1.784713 -3.079556  4.192865 -4.931510  0.221128  \n",
       "6   1.381391 -3.410147 -0.437894  6.170876  0.536590 -2.258617  \n",
       "7  -5.903622  1.555044  0.679583  3.990227 -4.600461  2.687322  \n",
       "8  -0.937141  1.817031 -3.414837  5.196090 -3.094033  1.823347  \n",
       "9   0.921460 -2.270914  2.296910 -2.945498 -0.863316 -2.363958  \n",
       "10  5.150886 -2.249004 -0.294797 -1.836554 -1.385168  3.488666  \n",
       "11  3.273017 -2.760325  1.566113 -2.771605  2.968455  1.412744  \n",
       "12  1.614968 -3.573758  5.654563 -7.952127  1.674620 -1.006084  \n",
       "13  4.306025 -3.230074 -1.223786  0.429879  0.768187  1.594597  \n",
       "14 -3.935016  2.851727  2.746354  0.898704 -6.315267  1.004400  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   1\n",
       "1   1\n",
       "2   1\n",
       "3   0\n",
       "4   0\n",
       "5   0\n",
       "6   1\n",
       "7   1\n",
       "8   0\n",
       "9   1\n",
       "10  0\n",
       "11  1\n",
       "12  1\n",
       "13  0\n",
       "14  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test evaluation using data preparation to the whole dataset and then split it into train and test datasets\n",
    "\n",
    "1. Data Preparation\n",
    "2. Split dataset into train and test sets\n",
    "3. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize the independent variables using the MinMaxScaler function from *sklearn.preprocessing*\n",
    "\n",
    "- feature_range: (min_val, max_val) The range of values each feature will have after the transformation\n",
    "- copy: True/False. If True, performs inplace row normalization\n",
    "\n",
    "The transformation is given by:\n",
    "\n",
    "- X_std = (X - X.min(axis=0))/(X.max(axis=0) - X.min(axis = 0))\n",
    "- X_scaled = X_std*(max - min) + min\n",
    "\n",
    "Where min, max = feature_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.478319</td>\n",
       "      <td>0.186936</td>\n",
       "      <td>0.424031</td>\n",
       "      <td>0.429320</td>\n",
       "      <td>0.585333</td>\n",
       "      <td>0.674249</td>\n",
       "      <td>0.529972</td>\n",
       "      <td>0.314806</td>\n",
       "      <td>0.393743</td>\n",
       "      <td>0.727494</td>\n",
       "      <td>0.517405</td>\n",
       "      <td>0.530177</td>\n",
       "      <td>0.279907</td>\n",
       "      <td>0.631903</td>\n",
       "      <td>0.744175</td>\n",
       "      <td>0.580887</td>\n",
       "      <td>0.511004</td>\n",
       "      <td>0.273137</td>\n",
       "      <td>0.340144</td>\n",
       "      <td>0.543896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.451084</td>\n",
       "      <td>0.734584</td>\n",
       "      <td>0.771313</td>\n",
       "      <td>0.240540</td>\n",
       "      <td>0.546150</td>\n",
       "      <td>0.642636</td>\n",
       "      <td>0.548818</td>\n",
       "      <td>0.891427</td>\n",
       "      <td>0.319694</td>\n",
       "      <td>0.370846</td>\n",
       "      <td>0.671665</td>\n",
       "      <td>0.476338</td>\n",
       "      <td>0.098575</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.794893</td>\n",
       "      <td>0.600236</td>\n",
       "      <td>0.329298</td>\n",
       "      <td>0.320941</td>\n",
       "      <td>0.628732</td>\n",
       "      <td>0.614195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.511375</td>\n",
       "      <td>0.385460</td>\n",
       "      <td>0.432530</td>\n",
       "      <td>0.404646</td>\n",
       "      <td>0.459618</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.430548</td>\n",
       "      <td>0.446149</td>\n",
       "      <td>0.443609</td>\n",
       "      <td>0.856237</td>\n",
       "      <td>0.296178</td>\n",
       "      <td>0.360230</td>\n",
       "      <td>0.618627</td>\n",
       "      <td>0.411667</td>\n",
       "      <td>0.370886</td>\n",
       "      <td>0.338365</td>\n",
       "      <td>0.522222</td>\n",
       "      <td>0.632484</td>\n",
       "      <td>0.496759</td>\n",
       "      <td>0.327452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.630259</td>\n",
       "      <td>0.405953</td>\n",
       "      <td>0.277012</td>\n",
       "      <td>0.643687</td>\n",
       "      <td>0.482764</td>\n",
       "      <td>0.568208</td>\n",
       "      <td>0.670650</td>\n",
       "      <td>0.345824</td>\n",
       "      <td>0.558990</td>\n",
       "      <td>0.476037</td>\n",
       "      <td>0.420609</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.568906</td>\n",
       "      <td>0.532053</td>\n",
       "      <td>0.811674</td>\n",
       "      <td>0.445949</td>\n",
       "      <td>0.375861</td>\n",
       "      <td>0.391518</td>\n",
       "      <td>0.327889</td>\n",
       "      <td>0.636582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.419431</td>\n",
       "      <td>0.243167</td>\n",
       "      <td>0.490262</td>\n",
       "      <td>0.483573</td>\n",
       "      <td>0.747615</td>\n",
       "      <td>0.538515</td>\n",
       "      <td>0.492948</td>\n",
       "      <td>0.473789</td>\n",
       "      <td>0.546354</td>\n",
       "      <td>0.542985</td>\n",
       "      <td>0.453421</td>\n",
       "      <td>0.490035</td>\n",
       "      <td>0.534418</td>\n",
       "      <td>0.235738</td>\n",
       "      <td>0.302497</td>\n",
       "      <td>0.444713</td>\n",
       "      <td>0.618642</td>\n",
       "      <td>0.540853</td>\n",
       "      <td>0.609732</td>\n",
       "      <td>0.377286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.444430</td>\n",
       "      <td>0.283784</td>\n",
       "      <td>0.450901</td>\n",
       "      <td>0.561128</td>\n",
       "      <td>0.462569</td>\n",
       "      <td>0.524357</td>\n",
       "      <td>0.412324</td>\n",
       "      <td>0.518873</td>\n",
       "      <td>0.357469</td>\n",
       "      <td>0.612032</td>\n",
       "      <td>0.522650</td>\n",
       "      <td>0.487317</td>\n",
       "      <td>0.529251</td>\n",
       "      <td>0.459356</td>\n",
       "      <td>0.531259</td>\n",
       "      <td>0.632045</td>\n",
       "      <td>0.336013</td>\n",
       "      <td>0.580416</td>\n",
       "      <td>0.188905</td>\n",
       "      <td>0.530207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.689532</td>\n",
       "      <td>0.714149</td>\n",
       "      <td>0.594112</td>\n",
       "      <td>0.683706</td>\n",
       "      <td>0.454431</td>\n",
       "      <td>0.699281</td>\n",
       "      <td>0.190466</td>\n",
       "      <td>0.575993</td>\n",
       "      <td>0.419974</td>\n",
       "      <td>0.560035</td>\n",
       "      <td>0.500425</td>\n",
       "      <td>0.071597</td>\n",
       "      <td>0.363462</td>\n",
       "      <td>0.524783</td>\n",
       "      <td>0.549336</td>\n",
       "      <td>0.293417</td>\n",
       "      <td>0.502443</td>\n",
       "      <td>0.650889</td>\n",
       "      <td>0.581766</td>\n",
       "      <td>0.381353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.567986</td>\n",
       "      <td>0.489223</td>\n",
       "      <td>0.482070</td>\n",
       "      <td>0.345210</td>\n",
       "      <td>0.345615</td>\n",
       "      <td>0.218900</td>\n",
       "      <td>0.509836</td>\n",
       "      <td>0.580219</td>\n",
       "      <td>0.619347</td>\n",
       "      <td>0.631058</td>\n",
       "      <td>0.599355</td>\n",
       "      <td>0.416775</td>\n",
       "      <td>0.448027</td>\n",
       "      <td>0.388136</td>\n",
       "      <td>0.045382</td>\n",
       "      <td>0.617074</td>\n",
       "      <td>0.572847</td>\n",
       "      <td>0.573196</td>\n",
       "      <td>0.212689</td>\n",
       "      <td>0.678249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.113426</td>\n",
       "      <td>0.502124</td>\n",
       "      <td>0.511154</td>\n",
       "      <td>0.652296</td>\n",
       "      <td>0.445349</td>\n",
       "      <td>0.437098</td>\n",
       "      <td>0.547311</td>\n",
       "      <td>0.550218</td>\n",
       "      <td>0.480702</td>\n",
       "      <td>0.370384</td>\n",
       "      <td>0.320106</td>\n",
       "      <td>0.636885</td>\n",
       "      <td>0.737566</td>\n",
       "      <td>0.402380</td>\n",
       "      <td>0.388947</td>\n",
       "      <td>0.634152</td>\n",
       "      <td>0.314889</td>\n",
       "      <td>0.616159</td>\n",
       "      <td>0.320920</td>\n",
       "      <td>0.626386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.407905</td>\n",
       "      <td>0.351658</td>\n",
       "      <td>0.486339</td>\n",
       "      <td>0.473384</td>\n",
       "      <td>0.795209</td>\n",
       "      <td>0.375725</td>\n",
       "      <td>0.482542</td>\n",
       "      <td>0.403967</td>\n",
       "      <td>0.523071</td>\n",
       "      <td>0.707425</td>\n",
       "      <td>0.698870</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.378659</td>\n",
       "      <td>0.517520</td>\n",
       "      <td>0.367678</td>\n",
       "      <td>0.674742</td>\n",
       "      <td>0.326088</td>\n",
       "      <td>0.481188</td>\n",
       "      <td>0.375029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.645980</td>\n",
       "      <td>0.378050</td>\n",
       "      <td>0.193560</td>\n",
       "      <td>0.775176</td>\n",
       "      <td>0.597252</td>\n",
       "      <td>0.736324</td>\n",
       "      <td>0.816450</td>\n",
       "      <td>0.197970</td>\n",
       "      <td>0.538437</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.464467</td>\n",
       "      <td>0.645771</td>\n",
       "      <td>0.554558</td>\n",
       "      <td>0.471819</td>\n",
       "      <td>0.810098</td>\n",
       "      <td>0.369106</td>\n",
       "      <td>0.511459</td>\n",
       "      <td>0.365598</td>\n",
       "      <td>0.443695</td>\n",
       "      <td>0.726352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.575181</td>\n",
       "      <td>0.780518</td>\n",
       "      <td>0.839212</td>\n",
       "      <td>0.456977</td>\n",
       "      <td>0.743798</td>\n",
       "      <td>0.748350</td>\n",
       "      <td>0.505014</td>\n",
       "      <td>0.491869</td>\n",
       "      <td>0.625129</td>\n",
       "      <td>0.764101</td>\n",
       "      <td>0.980308</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>0.367523</td>\n",
       "      <td>0.414490</td>\n",
       "      <td>0.680193</td>\n",
       "      <td>0.335776</td>\n",
       "      <td>0.628700</td>\n",
       "      <td>0.332283</td>\n",
       "      <td>0.756486</td>\n",
       "      <td>0.601738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.294549</td>\n",
       "      <td>0.329841</td>\n",
       "      <td>0.454661</td>\n",
       "      <td>0.420796</td>\n",
       "      <td>0.914858</td>\n",
       "      <td>0.394205</td>\n",
       "      <td>0.568826</td>\n",
       "      <td>0.328661</td>\n",
       "      <td>0.435775</td>\n",
       "      <td>0.666990</td>\n",
       "      <td>0.745701</td>\n",
       "      <td>0.265635</td>\n",
       "      <td>0.371488</td>\n",
       "      <td>0.380111</td>\n",
       "      <td>0.565494</td>\n",
       "      <td>0.282752</td>\n",
       "      <td>0.886282</td>\n",
       "      <td>0.147710</td>\n",
       "      <td>0.663529</td>\n",
       "      <td>0.456540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.730701</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.166979</td>\n",
       "      <td>0.736147</td>\n",
       "      <td>0.556886</td>\n",
       "      <td>0.397290</td>\n",
       "      <td>0.708981</td>\n",
       "      <td>0.426150</td>\n",
       "      <td>0.613812</td>\n",
       "      <td>0.581256</td>\n",
       "      <td>0.286875</td>\n",
       "      <td>0.413143</td>\n",
       "      <td>0.400546</td>\n",
       "      <td>0.307468</td>\n",
       "      <td>0.751653</td>\n",
       "      <td>0.305155</td>\n",
       "      <td>0.452931</td>\n",
       "      <td>0.446347</td>\n",
       "      <td>0.598405</td>\n",
       "      <td>0.612654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.490989</td>\n",
       "      <td>0.214670</td>\n",
       "      <td>0.227432</td>\n",
       "      <td>0.391887</td>\n",
       "      <td>0.371790</td>\n",
       "      <td>0.367532</td>\n",
       "      <td>0.566306</td>\n",
       "      <td>0.361706</td>\n",
       "      <td>0.519935</td>\n",
       "      <td>0.482849</td>\n",
       "      <td>0.280466</td>\n",
       "      <td>0.639148</td>\n",
       "      <td>0.439386</td>\n",
       "      <td>0.539050</td>\n",
       "      <td>0.181564</td>\n",
       "      <td>0.701598</td>\n",
       "      <td>0.703058</td>\n",
       "      <td>0.463050</td>\n",
       "      <td>0.089487</td>\n",
       "      <td>0.577226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.478319  0.186936  0.424031  0.429320  0.585333  0.674249  0.529972   \n",
       "1   0.451084  0.734584  0.771313  0.240540  0.546150  0.642636  0.548818   \n",
       "2   0.511375  0.385460  0.432530  0.404646  0.459618  0.370612  0.430548   \n",
       "3   0.630259  0.405953  0.277012  0.643687  0.482764  0.568208  0.670650   \n",
       "4   0.419431  0.243167  0.490262  0.483573  0.747615  0.538515  0.492948   \n",
       "5   0.444430  0.283784  0.450901  0.561128  0.462569  0.524357  0.412324   \n",
       "6   0.689532  0.714149  0.594112  0.683706  0.454431  0.699281  0.190466   \n",
       "7   0.567986  0.489223  0.482070  0.345210  0.345615  0.218900  0.509836   \n",
       "8   0.113426  0.502124  0.511154  0.652296  0.445349  0.437098  0.547311   \n",
       "9   0.407905  0.351658  0.486339  0.473384  0.795209  0.375725  0.482542   \n",
       "10  0.645980  0.378050  0.193560  0.775176  0.597252  0.736324  0.816450   \n",
       "11  0.575181  0.780518  0.839212  0.456977  0.743798  0.748350  0.505014   \n",
       "12  0.294549  0.329841  0.454661  0.420796  0.914858  0.394205  0.568826   \n",
       "13  0.730701  0.603800  0.166979  0.736147  0.556886  0.397290  0.708981   \n",
       "14  0.490989  0.214670  0.227432  0.391887  0.371790  0.367532  0.566306   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0   0.314806  0.393743  0.727494  0.517405  0.530177  0.279907  0.631903   \n",
       "1   0.891427  0.319694  0.370846  0.671665  0.476338  0.098575  0.283846   \n",
       "2   0.446149  0.443609  0.856237  0.296178  0.360230  0.618627  0.411667   \n",
       "3   0.345824  0.558990  0.476037  0.420609  0.612159  0.568906  0.532053   \n",
       "4   0.473789  0.546354  0.542985  0.453421  0.490035  0.534418  0.235738   \n",
       "5   0.518873  0.357469  0.612032  0.522650  0.487317  0.529251  0.459356   \n",
       "6   0.575993  0.419974  0.560035  0.500425  0.071597  0.363462  0.524783   \n",
       "7   0.580219  0.619347  0.631058  0.599355  0.416775  0.448027  0.388136   \n",
       "8   0.550218  0.480702  0.370384  0.320106  0.636885  0.737566  0.402380   \n",
       "9   0.403967  0.523071  0.707425  0.698870  0.245283  0.376471  0.378659   \n",
       "10  0.197970  0.538437  0.442200  0.464467  0.645771  0.554558  0.471819   \n",
       "11  0.491869  0.625129  0.764101  0.980308  0.185053  0.367523  0.414490   \n",
       "12  0.328661  0.435775  0.666990  0.745701  0.265635  0.371488  0.380111   \n",
       "13  0.426150  0.613812  0.581256  0.286875  0.413143  0.400546  0.307468   \n",
       "14  0.361706  0.519935  0.482849  0.280466  0.639148  0.439386  0.539050   \n",
       "\n",
       "          14        15        16        17        18        19  \n",
       "0   0.744175  0.580887  0.511004  0.273137  0.340144  0.543896  \n",
       "1   0.794893  0.600236  0.329298  0.320941  0.628732  0.614195  \n",
       "2   0.370886  0.338365  0.522222  0.632484  0.496759  0.327452  \n",
       "3   0.811674  0.445949  0.375861  0.391518  0.327889  0.636582  \n",
       "4   0.302497  0.444713  0.618642  0.540853  0.609732  0.377286  \n",
       "5   0.531259  0.632045  0.336013  0.580416  0.188905  0.530207  \n",
       "6   0.549336  0.293417  0.502443  0.650889  0.581766  0.381353  \n",
       "7   0.045382  0.617074  0.572847  0.573196  0.212689  0.678249  \n",
       "8   0.388947  0.634152  0.314889  0.616159  0.320920  0.626386  \n",
       "9   0.517520  0.367678  0.674742  0.326088  0.481188  0.375029  \n",
       "10  0.810098  0.369106  0.511459  0.365598  0.443695  0.726352  \n",
       "11  0.680193  0.335776  0.628700  0.332283  0.756486  0.601738  \n",
       "12  0.565494  0.282752  0.886282  0.147710  0.663529  0.456540  \n",
       "13  0.751653  0.305155  0.452931  0.446347  0.598405  0.612654  \n",
       "14  0.181564  0.701598  0.703058  0.463050  0.089487  0.577226  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "pd.DataFrame(X).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data into train and test sets using **train_test_split** from *sklearn.model_selection*, where its parameters are:\n",
    "\n",
    "- test_size: It should be between 0 and 1, where it represents the percentage of the full dataset that will be the test set\n",
    "- train_size: It should be between 0 and 1, if nothing is selected, then the value is automatically set to the complement of the test size\n",
    "- random_state: Seed to allow replicate the results\n",
    "\n",
    "Returns\n",
    "\n",
    "X_train, X_test, y_train, y_test arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 20) (330, 20) (670,) (330,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create our logistic regression using **LogisticRegression** from *sklearn.linear_model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting the model, we create the specific formula for the problem and then we can make a prediction using the test set. We can compare the prediction to the expected values and calculate a classification accuracy score using **accuracy_score** from *sklearn.metrics*\n",
    "\n",
    "- normalize: True/False. If False, return the number of correctly classified samples. Otherwise, return the fraction of correctly classificed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model with data leakaged: 84.848 %\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "accuracy_data_leakaged = accuracy_score(y_test, y_predicted)\n",
    "print(f\"Accuracy of the model with data leakaged: {round(accuracy_data_leakaged*100, 3)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We already know that there was data leakage, and this estimate of model accuracy is wrong**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test evaluation using data preparation to the whole dataset and then split it into train and test datasets\n",
    "\n",
    "1. Split dataset into train and test sets\n",
    "2. Data Preparation in the train and test sets individually\n",
    "3. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train) # Notice that we only fit the scale on the X_train model\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model without data leakaged: 85.455 %\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "accuracy_without_data_leakaged = accuracy_score(y_test, y_predicted)\n",
    "\n",
    "print(f\"Accuracy of the model without data leakaged: {round(accuracy_without_data_leakaged * 100,3)} %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this case, we can see that the estimate for the model is about 85.455%, which is more accurate than the estiamate with data leakage with just 84.848%. We would expect this to be an optimistic estimate with data leakage (better performance), although in this case, we can see that data leakage resulted in slightly worse performance. This might be because of the difficulty of the prediction task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model using Cross-Validation Evaluation\n",
    "\n",
    "We'll use the same dataset as in the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Evaluation using data preparation to the whole dataset first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-fold cross-validation procedure must first be defined. We'll use repeated stratified 10-fold cross-validation which is a best practice for classification. \n",
    "\n",
    "- Repeated means that the whole cross-validation procedure is repeated multiple times, three in this case. \n",
    "- Stratified means that each group of rows will have the relative composition of examples from each class as the whole dataset.\n",
    "\n",
    "We will use k = 10 or 10-fold cross-validation. This can be achieved using the RepeatedStratifiedKFold which can be configured to three repeats and 10 folds, and then using the cross_val_score() function to perform the procedure, passing in the defined model, cross-validation object, and metric to calculate, in this case, the accuracy\n",
    "\n",
    "The parameters for this **RepeatedStratifiedKFold** from *sklearn.model_selection* are:\n",
    "\n",
    "- n_splits: Number of folds, must be at least 2 (default: 5)\n",
    "- n_repeats: Number of times cross-validator needs to be repeated (default: 10)\n",
    "- random_state: Integer. Seed to replicate the results\n",
    "\n",
    "The parameters for this **cross_val_score** from *sklearn.model_selection* are: \n",
    "\n",
    "- estimator: Estimator object (function, it can be LogisticRegression, LinearRegression, etc) implementing \"fit\"\n",
    "- X: independent variables\n",
    "- y: target variable\n",
    "- n_jobs: Int (default None). Number of jobs to run in parallel. Training the estimator and computing the score are parallelized over the cross-validation splits. None means 1. -1 means using all processors\n",
    "- scoring: A str or a socrer callable object/function with signature scorer(estimator, X, y), which should return only a single value\n",
    "- cv: Int, cross-validation generator or an interable. Determines the cross-validation splitting strategy. Possible inputs are:\n",
    "    - None, to use the default 5-fold cross-validation\n",
    "    - int, to specify the number of folds in a (Stratified)KFold\n",
    "    - CV splitter,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "cross_val = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\n",
    "scores = cross_val_score(model, X, y, scoring=\"accuracy\", cv = cross_val, n_jobs = -1) #return a list with the score of each cross-validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86 0.91 0.88 0.81 0.83 0.84 0.81 0.84 0.88 0.84 0.84 0.86 0.85 0.83\n",
      " 0.89 0.87 0.79 0.97 0.84 0.84 0.81 0.88 0.8  0.85 0.89 0.88 0.87 0.83\n",
      " 0.83 0.87]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the accuracy getting data leakage:85.3 %, std = 3.607 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean of the accuracy getting data leakage:{round(100*scores.mean(),3)} %, std = {round(100*scores.std(),3)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation Evaluation with Correct Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation without data leakage when using cross-validation is slightly more challenging, it requires that the data preparation method is prepared on the training set and applied to the train and test sets within the cross-validation procedure. We can achieve this by defining a modeling pipeline that defiens a sequence of data preparation steps to performs and endning in the model to fit and evaluate.\n",
    "\n",
    "\n",
    "The evaluation procedure can be achieved if we create a pipeline class. This class takes a list of steps that define the pipeline. Each step in the list is a tuple with two elements.\n",
    "\n",
    "- The first element is the name of the step (a string)\n",
    "- The second element is the configured object of the step, such as a transform or a model. The model is only supported as the final step, although we can have as many transforms as we like in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [] #define the pipeline\n",
    "steps.append((\"scaler\", MinMaxScaler()))\n",
    "steps.append((\"model\", LogisticRegression()))\n",
    "\n",
    "pipeline = Pipeline(steps = steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86 0.91 0.87 0.81 0.83 0.84 0.81 0.84 0.88 0.84 0.84 0.86 0.85 0.83\n",
      " 0.89 0.88 0.8  0.97 0.84 0.84 0.81 0.88 0.81 0.85 0.89 0.88 0.87 0.84\n",
      " 0.84 0.87]\n"
     ]
    }
   ],
   "source": [
    "cross_val = RepeatedStratifiedKFold(n_splits = 10, n_repeats=3, random_state=1)\n",
    "scores = cross_val_score(pipeline, X, y, scoring = \"accuracy\", cv = cross_val, n_jobs = -1)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the accuracy getting data leakage:85.433 %, std = 3.471 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean of the accuracy getting data leakage:{round(100*scores.mean(),3)} %, std = {round(100*scores.std(),3)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this case, we can see that the model has an estimated accuracy of about 85.433% compared to the approach with the data leakage that achieved an accuracy of about 85.3%. As with the train-test example, removing data leakage has resulted in a slight improvement in performance when our intuition might suggest a drop given that data leakage often results in an optimistic estimate of model performance. Nevertheless, the examples demonstrate that data leakage may impact the estimate of model performance and how to correct data leakage by correctly performing data preparation after the data is split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
